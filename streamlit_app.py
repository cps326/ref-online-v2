from bs4 import BeautifulSoup
import pandas as pd
import re
import requests
from openai import OpenAI
import os
from collections import OrderedDict
import json
import streamlit as st
import io
import openai
import time
from urllib.parse import urljoin, urlparse

# =========================
# OpenAI API Key (Cloud ì¤‘ì‹¬)
# =========================
api_key = st.secrets.get("OPENAI_API_KEY") or os.environ.get("OPENAI_API_KEY")
if not api_key:
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Cloud > Secretsì— ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=api_key)
openai.api_key = api_key

st.set_page_config(layout="wide", page_title="KEI ì°¸ê³ ë¬¸í—Œ ì˜¨ë¼ì¸ìë£Œ ê²€ì¦ë„êµ¬ v.2")

# =========================
# ìµœì¢… ì¶œë ¥ ì»¬ëŸ¼ ìˆœì„œ(ìš”ì²­ ë°˜ì˜)
# =========================
FINAL_COL_ORDER = [
    "ìµœì¢…_URL_ìƒíƒœ",
    "ìµœì¢…_URL_ë©”ëª¨",
    "URL_ìˆ˜ì •ì•ˆ",
    "ì‘ì„±ê¸°ê´€_ì‘ì„±ì",
    "ì œëª©",
    "URL_ë³´ê³ ì„œê¸°ì¤€",
    "search_date",
    "ì›ë¬¸",
    "ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(ê·œì¹™ê¸°ë°˜)",
    "ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(GPTê¸°ë°˜)",
    # âœ… ê¸°ë³¸ì€ ë¹ˆ ì»¬ëŸ¼ ìœ ì§€(ì‹¤í—˜ ì˜µì…˜ ì‹¤í–‰ ì‹œì—ë§Œ ì±„ì›€)
    "URL_ë‚´ìš©ì¼ì¹˜ì—¬ë¶€(GPT)",
    # âœ… ìƒˆë¡œ ì¶”ê°€: ì‚¬ëŒì´ ë¹ ë¥´ê²Œ íŒë‹¨í•  ë©”íƒ€ ì •ë³´
    "í˜ì´ì§€_title",
    "í˜ì´ì§€_og_title",
    "í˜ì´ì§€_description",
    "íŒŒì¼_ì—¬ë¶€",
    "íŒŒì¼_í™•ì¥ì",
    "URL_ìƒíƒœ",
    "URL_ë©”ëª¨",
    "URL_ìƒíƒœì½”ë“œ",
    "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼",
    "ìˆ˜ë™ê²€ì¦_ë©”ëª¨",
]


def reorder_columns(df: pd.DataFrame, order: list[str]) -> pd.DataFrame:
    front = [c for c in order if c in df.columns]
    tail = [c for c in df.columns if c not in front]
    return df[front + tail]


def ensure_required_columns(df: pd.DataFrame) -> pd.DataFrame:
    if df is None:
        return df
    for c in FINAL_COL_ORDER:
        if c not in df.columns:
            df[c] = ""
    return df


# =========================
# íŒŒì¼ í™•ì¥ì íŒë³„
# =========================
DOC_EXTS = [".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".txt", ".csv", ".rtf"]


def detect_file_ext(url: str) -> str:
    if not isinstance(url, str):
        return ""
    lower = url.lower()
    for ext in DOC_EXTS:
        if ext in lower:
            return ext
    # ì¿¼ë¦¬ìŠ¤íŠ¸ë§ì— ë¶™ëŠ” ì¼€ì´ìŠ¤(?file=.pdf)ê¹Œì§€ëŠ” ì—¬ê¸°ì„œ ì™„ë²½íˆ ì¡ê¸° ì–´ë ¤ì›€
    return ""


# =========================
# URL ìƒíƒœ ì²´í¬ (ì •ìƒ/ì˜¤ë¥˜/í™•ì¸ë¶ˆê°€/ì •ìƒ(ë³´ì•ˆì£¼ì˜) + ë©”ëª¨)
# =========================
def check_url_status(url: str, timeout: int = 15) -> dict:
    if not isinstance(url, str) or not url.strip():
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "URL ì—†ìŒ"}

    url = url.strip()
    if not (url.startswith("http://") or url.startswith("https://")):
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "http/httpsë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ"}

    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        status_code = r.status_code
        final_url = r.url

        if 200 <= status_code < 300:
            return {"URL_ìƒíƒœ": "ì •ìƒ", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": ""}
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": f"HTTP {status_code}"}

    except requests.exceptions.SSLError:
        # SSL ê²€ì¦ ì‹¤íŒ¨ì§€ë§Œ verify=Falseë¡œ 1íšŒ ì¬ì‹œë„
        try:
            r2 = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)
            status_code = r2.status_code
            final_url = r2.url

            if 200 <= status_code < 300:
                memo = "SSL ê²€ì¦ ì‹¤íŒ¨(ë³´ì•ˆì£¼ì˜): verify=Falseë¡œëŠ” ì ‘ì†ë¨"
                return {"URL_ìƒíƒœ": "ì •ìƒ(ë³´ì•ˆì£¼ì˜)", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": memo}

            memo = f"SSL ê²€ì¦ ì‹¤íŒ¨ + HTTP {status_code}(verify=False)"
            return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": status_code, "URL_ìµœì¢…URL": final_url, "URL_ë©”ëª¨": memo}

        except Exception as e2:
            msg = f"{type(e2).__name__}: {str(e2)[:120]}"
            return {
                "URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€",
                "URL_ìƒíƒœì½”ë“œ": "",
                "URL_ìµœì¢…URL": "",
                "URL_ë©”ëª¨": f"SSL í•¸ë“œì…°ì´í¬ ì‹¤íŒ¨(verify=Falseë„ ì‹¤íŒ¨) - {msg}",
            }

    except requests.exceptions.Timeout:
        return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "Timeout"}
    except requests.exceptions.ConnectionError:
        return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "Connection error"}
    except requests.exceptions.InvalidURL:
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "Invalid URL"}
    except requests.exceptions.MissingSchema:
        return {"URL_ìƒíƒœ": "ì˜¤ë¥˜", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": "URL ìŠ¤í‚¤ë§ˆ ëˆ„ë½(http/https)"}
    except Exception as e:
        return {"URL_ìƒíƒœ": "í™•ì¸ë¶ˆê°€", "URL_ìƒíƒœì½”ë“œ": "", "URL_ìµœì¢…URL": "", "URL_ë©”ëª¨": f"ì˜ˆì™¸: {type(e).__name__}"}


# =========================
# ë©”íƒ€ ì •ë³´ ì¶”ì¶œ: title / og:title / meta description
# - ì‹¤íŒ¨í•´ë„ ë¹ˆê°’ ë°˜í™˜ (ì„±ëŠ¥/ì•ˆì • ëª©ì )
# =========================
def fetch_page_meta(url: str, timeout: int = 12) -> dict:
    if not isinstance(url, str) or not url.strip():
        return {"í˜ì´ì§€_title": "", "í˜ì´ì§€_og_title": "", "í˜ì´ì§€_description": ""}

    url = url.strip()
    if not (url.startswith("http://") or url.startswith("https://")):
        return {"í˜ì´ì§€_title": "", "í˜ì´ì§€_og_title": "", "í˜ì´ì§€_description": ""}

    headers = {"User-Agent": "Mozilla/5.0"}

    # íŒŒì¼ URLì´ë©´ ë©”íƒ€ ì¶”ì¶œ ì•ˆ í•¨(ë¶ˆí•„ìš” + ëŠë¦¼)
    if detect_file_ext(url):
        return {"í˜ì´ì§€_title": "", "í˜ì´ì§€_og_title": "", "í˜ì´ì§€_description": ""}

    try:
        r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        if not (200 <= r.status_code < 300):
            return {"í˜ì´ì§€_title": "", "í˜ì´ì§€_og_title": "", "í˜ì´ì§€_description": ""}

        soup = BeautifulSoup(r.text, "html.parser")

        title = (soup.title.string.strip() if soup.title and soup.title.string else "")
        og = soup.find("meta", property="og:title")
        og_title = og.get("content", "").strip() if og else ""
        desc = soup.find("meta", attrs={"name": "description"})
        description = desc.get("content", "").strip() if desc else ""

        # ë„ˆë¬´ ê¸¸ë©´ í™”ë©´/ì—‘ì…€ ë³´ê¸° í˜ë“œë‹ˆ ì»·
        return {
            "í˜ì´ì§€_title": title[:200],
            "í˜ì´ì§€_og_title": og_title[:200],
            "í˜ì´ì§€_description": description[:300],
        }
    except Exception:
        return {"í˜ì´ì§€_title": "", "í˜ì´ì§€_og_title": "", "í˜ì´ì§€_description": ""}


# =========================
# ì°¸ê³ ë¬¸í—Œ ë¶„ë¦¬ + ê·œì¹™ ê¸°ë°˜ í˜•ì‹ ì²´í¬(ê°„ë‹¨)
# =========================
def separator(entry):
    parts = [""] * 4
    if "http" in entry:
        pattern_http = r",\s+(?=http)"
    else:
        pattern_http = r",\s+(?=ê²€ìƒ‰ì¼)"

    parts_http = re.split(pattern_http, entry)
    doc_info = parts_http[0]
    ref_info = parts_http[1] if len(parts_http) > 1 else ""

    if "â€œ" in doc_info and "â€" in doc_info:
        match = re.match(r"(.+?),\s*?â€œ(.*)â€", doc_info)
        if match:
            parts[0] = match.group(1).strip()
            parts[1] = f"â€œ{match.group(2)}â€"
        else:
            parts[0] = doc_info.strip()
            parts[1] = ""
    else:
        parts[0] = doc_info.strip()
        parts[1] = ""

    if "http" in ref_info:
        pattern_ref = r",\s+(?=ê²€ìƒ‰ì¼)"
        parts_ref = re.split(pattern_ref, ref_info)
        parts[2] = parts_ref[0].strip()
        parts[3] = parts_ref[1].strip() if len(parts_ref) > 1 else ""
    else:
        parts[3] = ref_info.strip()

    return parts


def check_format(text):
    # ì œëª©(" ") ë˜ëŠ” â€œ â€ ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì¼ë‹¨ OKë¡œ ì²˜ë¦¬(ë³´ìˆ˜ì ìœ¼ë¡œ)
    if re.search(r'"[^"]*"', text):
        return True
    if re.search(r'â€œ[^â€]*â€', text):
        return True
    return False


# =========================
# GPT í˜•ì‹ ê²€ì¦ (í˜„ì¬ëŠ” ìœ ì§€: ë„ˆê°€ í”„ë¡¬í”„íŠ¸ ë°”ê¿€ ì˜ˆì •)
# =========================
def GPTcheck(doc):
    query = """
    ë‹¹ì‹ ì€ ê° ì¤„ë§ˆë‹¤ ì•„ë˜ í˜•ì‹ì— ë§ëŠ” ë¬¸í—Œ ì •ë³´ê°€ ì •í™•íˆ ì…ë ¥ë˜ì—ˆëŠ”ì§€ ê²€í† í•©ë‹ˆë‹¤.
    1. ì¶œì²˜
    2. ì œëª©: ë°˜ë“œì‹œ í°ë”°ì˜´í‘œ(" ")ë¡œ ê°ìŒˆ
    3. URL
    4. ê²€ìƒ‰ì¼: "ê²€ìƒ‰ì¼: yyyy.m.d." í˜•ì‹
    ì¶œë ¥: JSON {"ì˜¤ë¥˜ì—¬ë¶€":"X"} ë˜ëŠ” {"ì˜¤ë¥˜ì—¬ë¶€":"O(ì´ìœ )"}
    """
    retries = 0
    while retries < 5:
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system", "content": query},
                    {"role": "user", "content": f"ë¬¸ì„œ:{doc}"},
                ],
            )
            raw = response.choices[0].message.content
            result_dict = json.loads(raw)
            err = result_dict.get("ì˜¤ë¥˜ì—¬ë¶€") or "O(ì˜¤ë¥˜ì—¬ë¶€ ëˆ„ë½)"
            return {"ì˜¤ë¥˜ì—¬ë¶€": err, "ì›ë¬¸": doc}
        except openai.RateLimitError as e:
            time.sleep(getattr(e, "retry_after", 2) + 2)
            retries += 1
        except Exception as e:
            return {"ì˜¤ë¥˜ì—¬ë¶€": f"O(GPTcheck ì‹¤íŒ¨:{type(e).__name__})", "ì›ë¬¸": doc}


# =========================
# (ì‹¤í—˜ ì˜µì…˜) GPT URL ë‚´ìš©ì¼ì¹˜ ê²€ì‚¬ (ì„ íƒí•œ í–‰ë§Œ)
# - ê¸°ë³¸ ê¸°ëŠ¥ì—ì„œëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
# =========================
MAX_LEN = 20000  # ì‹¤í—˜ì´ë¼ ë” ì¤„ì—¬ì„œ ë¹„ìš©/ì‹œê°„ ì ˆê°

def crawling_for_gpt(url):
    # ì‹¤í—˜ì˜µì…˜ìš©: ë„ˆë¬´ ë¬´ê±°ìš´ iframe/ë¦¬ë‹¤ì´ë ‰íŠ¸ ë¡œì§ì€ ë°°ì œí•˜ê³  ë¹ ë¥´ê²Œ í…ìŠ¤íŠ¸ë§Œ
    headers = {"User-Agent": "Mozilla/5.0"}
    if not isinstance(url, str) or not url.startswith(("http://", "https://")):
        return "í™•ì¸ë¶ˆê°€"
    if detect_file_ext(url):
        return "íŒŒì¼(ë‚´ìš©í™•ì¸ë¶ˆê°€)"
    try:
        r = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
        if not (200 <= r.status_code < 300):
            return "í™•ì¸ë¶ˆê°€"
        soup = BeautifulSoup(r.text, "html.parser")
        txt = soup.get_text(" ", strip=True)
        return txt[:MAX_LEN]
    except Exception:
        return "í™•ì¸ë¶ˆê°€"


def gpt_url_match_single(info: str, url: str) -> str:
    page = crawling_for_gpt(url)
    if page in ("í™•ì¸ë¶ˆê°€", "íŒŒì¼(ë‚´ìš©í™•ì¸ë¶ˆê°€)"):
        return page

    retries = 0
    while retries < 3:
        try:
            resp = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ì›¹í˜ì´ì§€ ë‚´ìš©ì´ ì£¼ì–´ì§„ ì •ë³´ì™€ ëŒ€ì²´ë¡œ ì¼ì¹˜í•˜ë©´ 'ì¼ì¹˜(ìœ íš¨)', ì•„ë‹ˆë©´ 'ë¶ˆì¼ì¹˜(ì˜¤ë¥˜)'ë§Œ ì¶œë ¥í•˜ì„¸ìš”."},
                    {"role": "user", "content": f"[ì •ë³´]: {info}\n[ì›¹í˜ì´ì§€í…ìŠ¤íŠ¸]: {page}"},
                ],
            )
            out = (resp.choices[0].message.content or "").strip()
            if "ì¼ì¹˜" in out:
                return "ì¼ì¹˜(ìœ íš¨)"
            if "ë¶ˆì¼ì¹˜" in out:
                return "ë¶ˆì¼ì¹˜(ì˜¤ë¥˜)"
            return out[:50]
        except openai.RateLimitError as e:
            time.sleep(getattr(e, "retry_after", 2) + 2)
            retries += 1
        except Exception:
            return "í™•ì¸ë¶ˆê°€"
    return "í™•ì¸ë¶ˆê°€"


# =========================
# entries -> DataFrame
# =========================
def process_entries(entries):
    articles = []
    for entry in entries:
        rule_note = "" if check_format(entry) else "í™•ì¸í•„ìš”"

        s = separator(entry)
        s = ["í™•ì¸í•„ìš”" if item in ("NA", "", None) else item for item in s]

        ì‘ì„±ê¸°ê´€_ì‘ì„±ì = s[0]
        ì œëª© = s[1]
        URL_ë³´ê³ ì„œê¸°ì¤€ = s[2]

        search_date = s[3].replace("ê²€ìƒ‰ì¼: ", "").strip()
        if not re.search(r"\b\d{4}\.([1-9]|1[0-2])\.([1-9]|[12][0-9]|3[01])\b", search_date):
            search_date = "í™•ì¸í•„ìš”"

        url_result = check_url_status(URL_ë³´ê³ ì„œê¸°ì¤€)

        file_ext = detect_file_ext(URL_ë³´ê³ ì„œê¸°ì¤€ or "")
        is_file = "íŒŒì¼" if file_ext else "ì›¹"

        meta = fetch_page_meta(url_result.get("URL_ìµœì¢…URL") or URL_ë³´ê³ ì„œê¸°ì¤€)

        articles.append({
            "URL_ìƒíƒœ": url_result["URL_ìƒíƒœ"],
            "URL_ë©”ëª¨": url_result["URL_ë©”ëª¨"],
            "URL_ìƒíƒœì½”ë“œ": url_result["URL_ìƒíƒœì½”ë“œ"],
            "URL_ìˆ˜ì •ì•ˆ": url_result["URL_ìµœì¢…URL"],

            "íŒŒì¼_ì—¬ë¶€": is_file,
            "íŒŒì¼_í™•ì¥ì": file_ext,

            "í˜ì´ì§€_title": meta["í˜ì´ì§€_title"],
            "í˜ì´ì§€_og_title": meta["í˜ì´ì§€_og_title"],
            "í˜ì´ì§€_description": meta["í˜ì´ì§€_description"],

            "ì‘ì„±ê¸°ê´€_ì‘ì„±ì": ì‘ì„±ê¸°ê´€_ì‘ì„±ì,
            "ì œëª©": ì œëª©,
            "URL_ë³´ê³ ì„œê¸°ì¤€": URL_ë³´ê³ ì„œê¸°ì¤€,
            "search_date": search_date,

            "ì›ë¬¸": entry,
            "ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(ê·œì¹™ê¸°ë°˜)": rule_note,

            # ê¸°ë³¸ì€ ë¹„ì›€(ì‹¤í—˜ì˜µì…˜ìœ¼ë¡œë§Œ ì±„ì›€)
            "URL_ë‚´ìš©ì¼ì¹˜ì—¬ë¶€(GPT)": "",
            "ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(GPTê¸°ë°˜)": "",
        })

    df = pd.DataFrame(articles)
    return df


# =========================
# í™”ë©´/ì—‘ì…€ ìƒ‰ì¹  ê¸°ì¤€(ìµœì¢…_URL_ìƒíƒœ)
# =========================
def highlight_url_status(val):
    if val == "ì˜¤ë¥˜":
        return "background-color: #f8d7da"
    if val == "í™•ì¸ë¶ˆê°€":
        return "background-color: #fff3cd"
    if val == "ì •ìƒ(ë³´ì•ˆì£¼ì˜)":
        return "background-color: #ffe5b4"
    return ""


def write_excel_with_conditional_format(df: pd.DataFrame) -> bytes:
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="Sheet1")
        workbook = writer.book
        worksheet = writer.sheets["Sheet1"]

        if "ìµœì¢…_URL_ìƒíƒœ" in df.columns:
            status_col = df.columns.get_loc("ìµœì¢…_URL_ìƒíƒœ")

            fmt_red = workbook.add_format({"bg_color": "#F8D7DA"})
            fmt_yel = workbook.add_format({"bg_color": "#FFF3CD"})
            fmt_org = workbook.add_format({"bg_color": "#FFE5B4"})

            start_row = 1
            end_row = len(df)

            worksheet.conditional_format(start_row, status_col, end_row, status_col, {
                "type": "text", "criteria": "containing", "value": "ì˜¤ë¥˜", "format": fmt_red
            })
            worksheet.conditional_format(start_row, status_col, end_row, status_col, {
                "type": "text", "criteria": "containing", "value": "í™•ì¸ë¶ˆê°€", "format": fmt_yel
            })
            worksheet.conditional_format(start_row, status_col, end_row, status_col, {
                "type": "text", "criteria": "containing", "value": "ì •ìƒ(ë³´ì•ˆì£¼ì˜)", "format": fmt_org
            })

    output.seek(0)
    return output.read()


# =========================
# Streamlit UI
# =========================
def main():
    st.title("KEI ì°¸ê³ ë¬¸í—Œ ì˜¨ë¼ì¸ìë£Œ ê²€ì¦ë„êµ¬")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "processed_data" not in st.session_state:
        st.session_state["processed_data"] = None
    if "result_df" not in st.session_state:
        st.session_state["result_df"] = None

    # âœ… ì˜µì…˜: GPT URL ë‚´ìš©ì¼ì¹˜ ê¸°ë³¸ ì œê±° (ì‹¤í—˜ ì˜µì…˜ë§Œ ì œê³µ)
    st.subheader("âœ… ì‹¤í–‰ ì˜µì…˜(ì„ íƒ)")
    do_gpt_format = st.checkbox("GPTë¡œ ì°¸ê³ ë¬¸í—Œ ì‘ì„±ì–‘ì‹ ê²€í† í•˜ê¸°(ì„ íƒ)", value=False)
    st.caption("URL ë‚´ìš©ì¼ì¹˜(GPT)ëŠ” ê¸°ë³¸ì—ì„œ ì œê±°í–ˆìŠµë‹ˆë‹¤. í•„ìš” ì‹œ ì•„ë˜ â€˜ì‹¤í—˜ ê¸°ëŠ¥â€™ì—ì„œ ì¼ë¶€ í–‰ë§Œ ì„ íƒ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    uploaded_file = st.file_uploader(
        "ë³´ê³ ì„œ ì°¸ê³ ë¬¸í—Œ ì¤‘ ì˜¨ë¼ì¸ìë£Œì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼(txt)ë¥¼ ì—…ë¡œë“œ í•˜ê±°ë‚˜ ",
        type=["txt"],
    )
    text_data = st.text_area(
        "ë˜ëŠ” ì•„ë˜ì— ì˜¨ë¼ì¸ìë£Œì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
        "",
        height=300,
    )

    col_run, col_reset = st.columns([1, 1])
    with col_run:
        run_clicked = st.button("ğŸ‘‰ì—¬ê¸°ë¥¼ ëˆŒëŸ¬, ê²€ì¦ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")
    with col_reset:
        reset_clicked = st.button("ğŸ”ƒ(ê²€ì¦ í›„)ìˆ˜ë™ ì…ë ¥/ê²°ê³¼ ì´ˆê¸°í™” ë²„íŠ¼")

    if reset_clicked:
        st.session_state["processed_data"] = None
        st.session_state["result_df"] = None
        st.success("ì´ˆê¸°í™” ì™„ë£Œ! ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        st.stop()

    # âœ… Expander í—¤ë” ë°°ê²½ìƒ‰(ìˆ˜ë™ í™•ì¸ ì˜ì—­)
    st.markdown(
        """
        <style>
        div.manual-expander-marker + div[data-testid="stExpander"] details summary {
            background: #e8f0fe !important;
            border: 1px solid #8ab4f8 !important;
            border-radius: 12px !important;
            padding: 12px 14px !important;
            font-weight: 800 !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    if run_clicked:
        progress_bar = st.progress(0)
        status_text = st.empty()

        if not (uploaded_file or text_data.strip()):
            st.warning("í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            st.stop()

        progress_bar.progress(5)
        status_text.text("1ë‹¨ê³„: ì…ë ¥ ë°ì´í„° ë¡œë”© ì¤‘...")

        data = uploaded_file.read().decode("utf-8") if uploaded_file else text_data
        entries = data.strip().splitlines()

        progress_bar.progress(20)
        status_text.text("2ë‹¨ê³„: ê·œì¹™ê¸°ë°˜ ì‘ì„±ì–‘ì‹ + URL ìƒíƒœ/ìµœì¢…URL + ë©”íƒ€ ì •ë³´ ì¶”ì¶œ ì¤‘...")

        result_df = process_entries(entries)

        # ===== GPT í˜•ì‹ê²€ì¦(ì„ íƒ)
        if do_gpt_format:
            status_text.text("3ë‹¨ê³„: GPT ì‘ì„±ì–‘ì‹ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
            gpt_list = []
            n3 = len(entries)
            for idx, doc in enumerate(entries):
                gpt_list.append(GPTcheck(doc))
                progress = 20 + int(60 * (idx + 1) / max(n3, 1))  # 20~80
                progress_bar.progress(progress)

            result_df["ì°¸ê³ ë¬¸í—Œ_ì‘ì„±ì–‘ì‹_ì²´í¬(GPTê¸°ë°˜)"] = [
                r.get("ì˜¤ë¥˜ì—¬ë¶€", "O(ì˜¤ë¥˜ì—¬ë¶€ ì—†ìŒ)") if isinstance(r, dict) else "O(GPTcheck None)"
                for r in gpt_list
            ]
        else:
            progress_bar.progress(80)

        # ===== ìˆ˜ë™/ìµœì¢… ì»¬ëŸ¼ ì¤€ë¹„
        result_df["URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼"] = ""
        result_df["ìˆ˜ë™ê²€ì¦_ë©”ëª¨"] = ""
        result_df["ìµœì¢…_URL_ìƒíƒœ"] = result_df["URL_ìƒíƒœ"]
        result_df["ìµœì¢…_URL_ë©”ëª¨"] = result_df["URL_ë©”ëª¨"]

        result_df = ensure_required_columns(result_df)
        result_df = reorder_columns(result_df, FINAL_COL_ORDER)
        st.session_state["result_df"] = result_df

        progress_bar.progress(100)
        status_text.text("âœ… ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ì—ì„œ ìˆ˜ë™ í™•ì¸ í›„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")

    # =========================
    # ê²°ê³¼ í‘œì‹œ(ì„¸ì…˜ ê¸°ë°˜)
    # =========================
    if st.session_state["result_df"] is not None:
        result_df = ensure_required_columns(st.session_state["result_df"])
        result_df = reorder_columns(result_df, FINAL_COL_ORDER)

        # ë§ˆì»¤(ë‹¤ìŒ expander ìŠ¤íƒ€ì¼ ì ìš©ìš©)
        st.markdown('<div class="manual-expander-marker"></div>', unsafe_allow_html=True)

        with st.expander(
            "ğŸ” ë‹´ë‹¹ìì˜ ìˆ˜ë™ í™•ì¸(ì˜¤ë¥˜/í™•ì¸ë¶ˆê°€)ì´ í•„ìš”í•©ë‹ˆë‹¤. ì—¬ê¸°ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”! ì•„ë˜ í‘œê°€ í™œì„±í™”ë˜ë©´, URL(í´ë¦­)ì— ì ‘ì†í•˜ì—¬ ìµœì¢… íŒì • ê²°ê³¼ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.ğŸ¤—",
            expanded=False,
        ):
            issue_mask = result_df["URL_ìƒíƒœ"].isin(["ì˜¤ë¥˜", "í™•ì¸ë¶ˆê°€"])
            issues_cols = [
                "URL_ìƒíƒœ", "URL_ë©”ëª¨", "URL_ë³´ê³ ì„œê¸°ì¤€", "URL_ìˆ˜ì •ì•ˆ",
                "í˜ì´ì§€_title", "í˜ì´ì§€_og_title", "í˜ì´ì§€_description",
                "ì‘ì„±ê¸°ê´€_ì‘ì„±ì", "ì œëª©",
                "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼", "ìˆ˜ë™ê²€ì¦_ë©”ëª¨"
            ]
            issues_df = result_df.loc[issue_mask, [c for c in issues_cols if c in result_df.columns]].copy()

            if len(issues_df) == 0:
                st.info("ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•œ(ì˜¤ë¥˜/í™•ì¸ë¶ˆê°€) í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                edited = st.data_editor(
                    issues_df,
                    use_container_width=True,
                    column_config={
                        "URL_ë³´ê³ ì„œê¸°ì¤€": st.column_config.LinkColumn("URL(í´ë¦­)", display_text="ì—´ê¸°"),
                        "URL_ìˆ˜ì •ì•ˆ": st.column_config.LinkColumn("ë¦¬ë‹¤ì´ë ‰íŠ¸ ìµœì¢… URL(í´ë¦­)", display_text="ì—´ê¸°"),
                        "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼": st.column_config.SelectboxColumn(
                            "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼(ì„ íƒ)",
                            options=["", "ì •ìƒ", "ì •ìƒ(ë³´ì•ˆì£¼ì˜)", "ì˜¤ë¥˜", "í™•ì¸ë¶ˆê°€"],
                        ),
                        "ìˆ˜ë™ê²€ì¦_ë©”ëª¨": st.column_config.TextColumn("ìˆ˜ë™ê²€ì¦_ë©”ëª¨"),
                    },
                    disabled=[c for c in ["URL_ìƒíƒœ", "URL_ë©”ëª¨", "ì‘ì„±ê¸°ê´€_ì‘ì„±ì", "ì œëª©", "í˜ì´ì§€_title", "í˜ì´ì§€_og_title", "í˜ì´ì§€_description"] if c in issues_df.columns],
                    key="manual_editor",
                )

                if st.button("âœ… ìˆ˜ë™ íŒì • ì ìš©"):
                    result_df.loc[edited.index, "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼"] = edited.get("URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼", "")
                    result_df.loc[edited.index, "ìˆ˜ë™ê²€ì¦_ë©”ëª¨"] = edited.get("ìˆ˜ë™ê²€ì¦_ë©”ëª¨", "")

                    has_manual = result_df["URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼"].astype(str).str.strip().ne("")
                    result_df.loc[has_manual, "ìµœì¢…_URL_ìƒíƒœ"] = result_df.loc[has_manual, "URL_ìˆ˜ë™ê²€ì¦_ê²°ê³¼"]

                    has_manual_memo = result_df["ìˆ˜ë™ê²€ì¦_ë©”ëª¨"].astype(str).str.strip().ne("")
                    result_df.loc[has_manual_memo, "ìµœì¢…_URL_ë©”ëª¨"] = result_df.loc[has_manual_memo, "ìˆ˜ë™ê²€ì¦_ë©”ëª¨"]

                    result_df = reorder_columns(result_df, FINAL_COL_ORDER)
                    st.session_state["result_df"] = result_df
                    st.success("ìˆ˜ë™ íŒì •ì„ ìµœì¢… ê°’ì— ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.")

        # =========================
        # âœ… ì‹¤í—˜ ê¸°ëŠ¥: ì„ íƒí•œ í–‰ë§Œ GPT URL ë‚´ìš©ì¼ì¹˜ ê²€ì‚¬
        # =========================
        with st.expander("ğŸ§ª (ì‹¤í—˜) ì„ íƒí•œ í–‰ë§Œ GPTë¡œ URL ë‚´ìš©ì¼ì¹˜ ê²€í† í•˜ê¸° (ê¸°ë³¸ ë¹„í™œì„±)", expanded=False):
            st.caption("âš ï¸ ì´ ê¸°ëŠ¥ì€ ì‹¤í—˜ìš©ì…ë‹ˆë‹¤. ì„ íƒí•œ ì¼ë¶€ í–‰ë§Œ GPTê°€ í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  'ì¼ì¹˜/ë¶ˆì¼ì¹˜'ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.")
            st.caption("ë¹„ìš©/ì‹œê°„ì´ ë“¤ ìˆ˜ ìˆìœ¼ë‹ˆ, ê¼­ í•„ìš”í•œ í•­ëª©ë§Œ ì„ íƒí•´ì„œ ì‹¤í–‰í•˜ì„¸ìš”.")

            selectable_cols = ["ì‘ì„±ê¸°ê´€_ì‘ì„±ì", "ì œëª©", "URL_ë³´ê³ ì„œê¸°ì¤€", "URL_ìˆ˜ì •ì•ˆ", "í˜ì´ì§€_title", "í˜ì´ì§€_description", "URL_ë‚´ìš©ì¼ì¹˜ì—¬ë¶€(GPT)"]
            view_df = result_df[[c for c in selectable_cols if c in result_df.columns]].copy()
            view_df.insert(0, "ì„ íƒ", False)

            edited_sel = st.data_editor(
                view_df,
                use_container_width=True,
                column_config={
                    "URL_ë³´ê³ ì„œê¸°ì¤€": st.column_config.LinkColumn("URL(í´ë¦­)", display_text="ì—´ê¸°"),
                    "URL_ìˆ˜ì •ì•ˆ": st.column_config.LinkColumn("ìµœì¢… URL(í´ë¦­)", display_text="ì—´ê¸°"),
                    "ì„ íƒ": st.column_config.CheckboxColumn("ì„ íƒ"),
                },
                key="gpt_urlmatch_selector",
            )

            if st.button("ğŸ§ª ì„ íƒí•œ í–‰ë§Œ GPT URL ë‚´ìš©ì¼ì¹˜ ì‹¤í–‰"):
                selected_idx = edited_sel.index[edited_sel["ì„ íƒ"] == True].tolist()
                if not selected_idx:
                    st.warning("ì„ íƒëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € â€˜ì„ íƒâ€™ ì²´í¬ë°•ìŠ¤ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                else:
                    prog = st.progress(0)
                    for k, idx in enumerate(selected_idx):
                        info = f"{result_df.loc[idx, 'ì œëª©']} + {result_df.loc[idx, 'ì‘ì„±ê¸°ê´€_ì‘ì„±ì']}"
                        # ìµœì¢… URLì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©
                        url = result_df.loc[idx, "URL_ìˆ˜ì •ì•ˆ"] or result_df.loc[idx, "URL_ë³´ê³ ì„œê¸°ì¤€"]
                        result_df.loc[idx, "URL_ë‚´ìš©ì¼ì¹˜ì—¬ë¶€(GPT)"] = gpt_url_match_single(info, url)
                        prog.progress(int(100 * (k + 1) / len(selected_idx)))
                    st.session_state["result_df"] = reorder_columns(result_df, FINAL_COL_ORDER)
                    st.success("ì„ íƒí•œ í–‰ì— ëŒ€í•´ GPT URL ë‚´ìš©ì¼ì¹˜ ê²°ê³¼ë¥¼ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤(ì‹¤í—˜).")

        # ë©”ì¸ í‘œ
        styled = result_df.style.applymap(highlight_url_status, subset=["ìµœì¢…_URL_ìƒíƒœ"])
        st.dataframe(styled, use_container_width=True)

        # ì—‘ì…€
        excel_bytes = write_excel_with_conditional_format(result_df)
        st.session_state["processed_data"] = excel_bytes

        st.download_button(
            label="ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ",
            data=st.session_state["processed_data"],
            file_name="result.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )


if __name__ == "__main__":
    main()
